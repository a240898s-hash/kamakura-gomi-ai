import streamlit as st
import os
import faiss
import pickle
import numpy as np
import datetime
from sentence_transformers import SentenceTransformer
from huggingface_hub import InferenceClient

# ==========================================
# âš¡ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
HF_TOKEN = "HF_TOKEN"  # â† ã‚ãªãŸã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ›¸ãæ›ãˆã‚‹ï¼
STORE_DIR = "vector_store"
MODEL_NAME = "intfloat/multilingual-e5-small"
CHAT_MODEL = "Qwen/Qwen2.5-7B-Instruct"

# éŒå€‰å¸‚ã®åœ°åŒºãƒªã‚¹ãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸ã°ã›ã‚‹ç”¨ï¼‰
AREA_LIST = [
    "ä»Šæ³‰", "å¤§èˆ¹", "å²©ç€¬", "å°è¢‹è°·", "é«˜é‡", 
    "å±±ãƒå†…", "å°", "å°å‚", "æ´¥", "è…°è¶Š", 
    "ä¸ƒé‡Œã‚¬æµœ", "æ¥µæ¥½å¯º", "é•·è°·", "å‚ãƒä¸‹", 
    "ç”±æ¯”ã‚¬æµœ", "ææœ¨åº§", "éŒå€‰å±±", "ç¬›ç”°",
    "æ‰‹åºƒ", "å¸¸ç›¤", "æ¢¶åŸ", "å¯ºåˆ†", "ä¸Šç”ºå±‹"
]

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="éŒå€‰å¸‚ã‚´ãƒŸå‡ºã—AI", page_icon="ğŸ—‘ï¸")

# --- é–¢æ•°å®šç¾© ---
@st.cache_resource
def load_models():
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–ï¼‰"""
    if not os.path.exists(os.path.join(STORE_DIR, "index.faiss")):
        return None, None, None
        
    with open(os.path.join(STORE_DIR, "doc_map.pkl"), "rb") as f:
        doc_map = pickle.load(f)
    index = faiss.read_index(os.path.join(STORE_DIR, "index.faiss"))
    encoder = SentenceTransformer(MODEL_NAME)
    return index, doc_map, encoder

def get_date_info():
    weekdays = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
    now = datetime.datetime.now()
    today_str = f"{now.strftime('%Yå¹´%mæœˆ%dæ—¥')}ï¼ˆ{weekdays[now.weekday()]}æ›œæ—¥ï¼‰"
    tomorrow = now + datetime.timedelta(days=1)
    tomorrow_str = f"{tomorrow.strftime('%Yå¹´%mæœˆ%dæ—¥')}ï¼ˆ{weekdays[tomorrow.weekday()]}æ›œæ—¥ï¼‰"
    return today_str, tomorrow_str

def generate_response(user_input, area, index, doc_map, encoder):
    # æ¤œç´¢
    search_query = f"{area} {user_input}"
    query_vector = encoder.encode([f"query: {search_query}"], normalize_embeddings=True)
    distances, indices = index.search(np.array(query_vector), 3)
    
    results = []
    for idx in indices[0]:
        if idx < len(doc_map):
            results.append(doc_map[idx])
    
    # å›ç­”ç”Ÿæˆ
    client = InferenceClient(api_key=HF_TOKEN)
    today, tomorrow = get_date_info()
    context_str = "\n".join(results)
    
    system_instruction = (
        "ã‚ãªãŸã¯éŒå€‰å¸‚ã®ã‚´ãƒŸå‡ºã—æ¡ˆå†…ä¿‚ã§ã™ã€‚\n"
        f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½ã‚€åœ°åŒº: {area}ã€‘\n"
        f"ã€ä»Šæ—¥: {today}ã€‘\n"
        f"ã€æ˜æ—¥: {tomorrow}ã€‘\n"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€ä¸Šè¨˜ã®ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½ã‚€åœ°åŒºã€‘ã®ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        "ä»–ã®åœ°åŒºã®æƒ…å ±ã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚"
        "æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œè³‡æ–™ã«ãªã„ã®ã§ã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚"
    )
    
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": f"ã€å‚è€ƒæƒ…å ±ã€‘\n{context_str}\n\nã€è³ªå•ã€‘\n{user_input}"}
    ]
    
    try:
        response = client.chat_completion(
            model=CHAT_MODEL, messages=messages, max_tokens=500, temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"

# --- ã‚¢ãƒ—ãƒªã®ç”»é¢ ---
st.title("ğŸ—‘ï¸ éŒå€‰å¸‚ã‚´ãƒŸå‡ºã—AIãƒãƒ£ãƒƒãƒˆ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆè¨­å®šã‚¨ãƒªã‚¢ï¼‰
st.sidebar.header("âš™ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š")
selected_area = st.sidebar.selectbox("ã‚ãªãŸã®ãŠä½ã¾ã„ã®åœ°åŒºã‚’é¸ã‚“ã§ãã ã•ã„", AREA_LIST)
st.sidebar.write(f"ç¾åœ¨ã®è¨­å®š: **{selected_area}** åœ°åŒº")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
index, doc_map, encoder = load_models()

if index is None:
    st.error("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« build_index.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
else:
    # éå»ã®ãƒãƒ£ãƒƒãƒˆã‚’è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šæ˜æ—¥ã®ã‚´ãƒŸã¯ï¼Ÿï¼‰"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AIã®å›ç­”ã‚’ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("AIãŒè€ƒãˆä¸­..."):
                response = generate_response(prompt, selected_area, index, doc_map, encoder)
                st.markdown(response)
        
        # å±¥æ­´ã«è¿½åŠ 
        st.session_state.messages.append({"role": "assistant", "content": response})